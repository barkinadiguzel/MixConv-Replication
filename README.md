# ðŸ± MixConv-Replication â€” Mixed Depthwise Convolutional Kernels

This repository provides a **faithful PyTorch replication** of  
**Mixed Depthwise Convolutions (MixConv)**, a convolutional design that  
captures **multiple receptive field sizes within a single layer** by  
mixing kernel sizes **across channel groups**.

The goal of this project is **architectural fidelity and conceptual clarity** â€”  
translating the paperâ€™s **design intuition, mathematical formulation, and block structure**  
into clean, modular code, **without training, datasets, or empirical benchmarking**.

Core properties:
- **Channel-wise kernel mixing** instead of multi-branch fusion âŸ¡
- **Depthwise convolutions with heterogeneous kernel sizes** âŸ¢
- **Fixed, deterministic channel partitioning** (no attention, no gating) âŸ£
- Minimal computational overhead with MobileNet-style compatibility âŸ¤

**Paper reference:**  [MixConv: Mixed Depthwise Convolutional Kernels (Tan & Le, BMVC 2019)](https://arxiv.org/abs/1907.09595) â§‰


---

## âŸ¡ Overview â€” Mixing Receptive Fields Inside a Single Layer

![MixConv Overview](images/figmix.jpg)

> In standard CNNs, each convolutional layer is restricted to  
> **one kernel size**, enforcing a uniform receptive field across channels.  
> **MixConv breaks this constraint by distributing multiple kernel sizes  
> across channel groups within the same depthwise convolution.**

Instead of branching the network (as in multi-path architectures),  
MixConv operates **within a single layer**:

- Input channels are partitioned into disjoint groups
- Each group is convolved with a different kernel size
- Outputs are concatenated along the channel dimension

This design allows the network to represent **multiple spatial scales simultaneously**,  
while preserving the efficiency of depthwise convolutions.

Crucially, receptive field diversity is achieved **structurally**,  
not dynamically â€” there is **no input-conditioned selection or attention**.

---

## ðŸ§® Mixed Depthwise Convolution Formulation

Given an input feature map

$$
X \in \mathbb{R}^{C \times H \times W},
$$

MixConv defines a set of kernel sizes

$$
\mathcal{K} = \{k_1, k_2, \dots, k_M\},
$$

and partitions the channel dimension such that

$$
C = \sum_{m=1}^{M} C_m.
$$

---

### Channel Partitioning

The input tensor is split along channels:

$$
X = \text{Concat}(X^{(1)}, X^{(2)}, \dots, X^{(M)}),
$$

where

$$
X^{(m)} \in \mathbb{R}^{C_m \times H \times W}.
$$

The partitioning is **fixed and deterministic**,  
typically using equal or logarithmic allocation across kernel sizes.

---

### Mixed Depthwise Convolution

Each channel group is processed independently using a depthwise convolution  
with its assigned kernel size:

$$
Y^{(m)} = X^{(m)} * K^{(m)}, \quad K^{(m)} \in \mathbb{R}^{k_m \times k_m}.
$$

All convolutions are depthwise:

- No cross-channel mixing
- One kernel per channel

The final output is obtained via channel-wise concatenation:

$$
Y = \text{Concat}(Y^{(1)}, Y^{(2)}, \dots, Y^{(M)}),
$$

yielding

$$
Y \in \mathbb{R}^{C \times H \times W}.
$$

---

## âš™ Architectural Interpretation

- **Receptive field diversity is encoded structurally**
- No soft attention, no competition between scales
- Different channels specialize in different spatial extents
- Fully compatible with pointwise ($$1 \times 1$$) convolutions
- Designed for efficient mobile and embedded architectures

MixConv is typically followed by a pointwise convolution to enable  
**cross-channel interaction**, mirroring the MobileNet design philosophy.

---

## ðŸ§© Repository Structure

```bash
MixConv-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ kernels/
â”‚   â”‚   â””â”€â”€ kernel_sizes.py
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ depthwise_conv.py
â”‚   â”‚   â””â”€â”€ mixconv.py
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â”œâ”€â”€ mixconv_block.py
â”‚   â”‚   â””â”€â”€ inverted_residual.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ mobilenet_stub.py
â”‚   â”‚   â””â”€â”€ mixnet_stub.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ðŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
